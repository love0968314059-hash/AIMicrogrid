"""
å¼ºåŒ–å­¦ä¹ èƒ½é‡ç®¡ç†æ¨¡å—
====================

åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”èƒ½é‡ç®¡ç†ç­–ç•¥ç”Ÿæˆç³»ç»Ÿã€‚
æ”¯æŒDQNã€PPOã€SACç­‰ç®—æ³•ã€‚
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Callable
from dataclasses import dataclass, field
from collections import deque
import json


@dataclass
class Experience:
    """ç»éªŒå›æ”¾æ ·æœ¬"""
    state: np.ndarray
    action: np.ndarray
    reward: float
    next_state: np.ndarray
    done: bool


class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, capacity: int = 100000):
        self.buffer = deque(maxlen=capacity)
        
    def push(self, experience: Experience):
        self.buffer.append(experience)
        
    def sample(self, batch_size: int) -> List[Experience]:
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        return [self.buffer[i] for i in indices]
    
    def __len__(self):
        return len(self.buffer)


class NeuralNetwork:
    """ç®€å•ç¥ç»ç½‘ç»œå®ç°ï¼ˆä¸ä¾èµ–æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼‰"""
    
    def __init__(self, layer_sizes: List[int], activation: str = 'relu'):
        self.layer_sizes = layer_sizes
        self.activation = activation
        self.weights = []
        self.biases = []
        
        # Xavieråˆå§‹åŒ–
        for i in range(len(layer_sizes) - 1):
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])
            b = np.zeros((1, layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)
            
    def _activate(self, x: np.ndarray, derivative: bool = False) -> np.ndarray:
        if self.activation == 'relu':
            if derivative:
                return (x > 0).astype(float)
            return np.maximum(0, x)
        elif self.activation == 'tanh':
            if derivative:
                return 1 - np.tanh(x) ** 2
            return np.tanh(x)
        elif self.activation == 'sigmoid':
            s = 1 / (1 + np.exp(-np.clip(x, -500, 500)))
            if derivative:
                return s * (1 - s)
            return s
        return x
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.layer_outputs = [x]
        current = x
        
        for i, (w, b) in enumerate(zip(self.weights, self.biases)):
            z = np.dot(current, w) + b
            if i < len(self.weights) - 1:
                current = self._activate(z)
            else:
                current = z  # è¾“å‡ºå±‚ä¸æ¿€æ´»
            self.layer_outputs.append(current)
            
        return current
    
    def backward(self, loss_gradient: np.ndarray, learning_rate: float = 0.001) -> None:
        """åå‘ä¼ æ’­"""
        gradients_w = []
        gradients_b = []
        
        delta = loss_gradient
        
        for i in range(len(self.weights) - 1, -1, -1):
            gradients_w.insert(0, np.dot(self.layer_outputs[i].T, delta))
            gradients_b.insert(0, np.sum(delta, axis=0, keepdims=True))
            
            if i > 0:
                delta = np.dot(delta, self.weights[i].T) * self._activate(
                    self.layer_outputs[i], derivative=True
                )
        
        # æ›´æ–°æƒé‡
        for i in range(len(self.weights)):
            self.weights[i] -= learning_rate * gradients_w[i]
            self.biases[i] -= learning_rate * gradients_b[i]
    
    def get_params(self) -> Dict:
        return {
            'weights': [w.tolist() for w in self.weights],
            'biases': [b.tolist() for b in self.biases]
        }
    
    def set_params(self, params: Dict):
        self.weights = [np.array(w) for w in params['weights']]
        self.biases = [np.array(b) for b in params['biases']]


class EnergyManagementAgent:
    """å¼ºåŒ–å­¦ä¹ èƒ½é‡ç®¡ç†æ™ºèƒ½ä½“"""
    
    def __init__(self, 
                 state_dim: int = 10,
                 action_dim: int = 2,
                 hidden_dims: List[int] = [128, 64],
                 learning_rate: float = 0.001,
                 gamma: float = 0.99,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 buffer_size: int = 100000,
                 batch_size: int = 64):
        """
        åˆå§‹åŒ–èƒ½é‡ç®¡ç†æ™ºèƒ½ä½“
        
        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦
            learning_rate: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            epsilon: æ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢ç‡è¡°å‡
            epsilon_min: æœ€å°æ¢ç´¢ç‡
            buffer_size: ç»éªŒå›æ”¾å®¹é‡
            batch_size: æ‰¹é‡å¤§å°
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.batch_size = batch_size
        
        # æ„å»ºç½‘ç»œ
        layer_sizes = [state_dim] + hidden_dims + [action_dim * 11]  # ç¦»æ•£åŒ–åŠ¨ä½œ
        self.q_network = NeuralNetwork(layer_sizes)
        self.target_network = NeuralNetwork(layer_sizes)
        self._update_target_network()
        
        # ç»éªŒå›æ”¾
        self.replay_buffer = ReplayBuffer(buffer_size)
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_steps = 0
        self.episode_rewards = []
        self.losses = []
        
        # åŠ¨ä½œç©ºé—´ç¦»æ•£åŒ–
        self.action_bins = np.linspace(-1, 1, 11)  # ç”µæ± åŠ¨ä½œ
        
    def _update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.set_params(self.q_network.get_params())
        
    def _discretize_action(self, continuous_action: np.ndarray) -> int:
        """å°†è¿ç»­åŠ¨ä½œè½¬æ¢ä¸ºç¦»æ•£ç´¢å¼•"""
        battery_idx = np.argmin(np.abs(self.action_bins - continuous_action[0]))
        diesel_idx = 1 if continuous_action[1] > 0.5 else 0
        return battery_idx * 2 + diesel_idx
    
    def _continuous_action(self, discrete_idx: int) -> np.ndarray:
        """å°†ç¦»æ•£ç´¢å¼•è½¬æ¢ä¸ºè¿ç»­åŠ¨ä½œ"""
        battery_idx = discrete_idx // 2
        diesel_idx = discrete_idx % 2
        return np.array([self.action_bins[battery_idx], float(diesel_idx)])
    
    def select_action(self, state: np.ndarray, training: bool = True) -> Dict:
        """
        é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: çŠ¶æ€å‘é‡
            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
            
        Returns:
            åŠ¨ä½œå­—å…¸
        """
        if training and np.random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºåŠ¨ä½œ
            battery_action = np.random.uniform(-1, 1)
            diesel_on = np.random.random() > 0.7
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            state_input = state.reshape(1, -1)
            q_values = self.q_network.forward(state_input)
            best_action_idx = np.argmax(q_values[0])
            continuous_action = self._continuous_action(best_action_idx)
            battery_action = continuous_action[0]
            diesel_on = continuous_action[1] > 0.5
        
        return {
            'battery_action': float(battery_action),
            'diesel_on': bool(diesel_on)
        }
    
    def calculate_reward(self, state: Dict, action: Dict, next_state: Dict) -> float:
        """
        è®¡ç®—å¥–åŠ±å‡½æ•°
        
        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            next_state: ä¸‹ä¸€çŠ¶æ€
            
        Returns:
            å¥–åŠ±å€¼
        """
        reward = 0.0
        
        # 1. æˆæœ¬æƒ©ç½š
        cost = next_state.get('cost', 0)
        reward -= cost * 10  # æ”¾å¤§æˆæœ¬å½±å“
        
        # 2. å¯å†ç”Ÿèƒ½æºåˆ©ç”¨å¥–åŠ±
        renewable_ratio = next_state.get('renewable_ratio', 0)
        reward += renewable_ratio * 5
        
        # 3. ç”µæ± SOCç®¡ç†
        soc = next_state.get('battery_soc', 0.5)
        if 0.3 <= soc <= 0.7:
            reward += 1.0  # SOCåœ¨å¥åº·èŒƒå›´
        elif soc < 0.15 or soc > 0.85:
            reward -= 2.0  # SOCè¿‡ä½æˆ–è¿‡é«˜
        
        # 4. ç”µç½‘äº¤äº’æƒ©ç½š
        grid_power = abs(next_state.get('grid_power', 0))
        reward -= grid_power * 0.01  # é¼“åŠ±è‡ªç»™è‡ªè¶³
        
        # 5. æŸ´æ²¹æœºä½¿ç”¨æƒ©ç½š
        if action.get('diesel_on', False):
            reward -= 0.5
        
        # 6. ä¾›éœ€å¹³è¡¡
        power_balance = next_state.get('power_balance', {})
        imbalance = abs(power_balance.get('generation', 0) - 
                       power_balance.get('consumption', 0))
        if imbalance > 50:
            reward -= imbalance * 0.02
        
        return reward
    
    def train_step(self, state: np.ndarray, action: Dict, 
                   reward: float, next_state: np.ndarray, done: bool):
        """
        è®­ç»ƒæ­¥éª¤
        
        Args:
            state: å½“å‰çŠ¶æ€
            action: åŠ¨ä½œ
            reward: å¥–åŠ±
            next_state: ä¸‹ä¸€çŠ¶æ€
            done: æ˜¯å¦ç»“æŸ
        """
        # è½¬æ¢åŠ¨ä½œæ ¼å¼
        action_array = np.array([
            action['battery_action'],
            1.0 if action['diesel_on'] else 0.0
        ])
        
        # å­˜å‚¨ç»éªŒ
        exp = Experience(state, action_array, reward, next_state, done)
        self.replay_buffer.push(exp)
        
        # ç»éªŒå›æ”¾è®­ç»ƒ
        if len(self.replay_buffer) >= self.batch_size:
            self._train_batch()
            
        # æ›´æ–°æ¢ç´¢ç‡
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.training_steps += 1
        if self.training_steps % 100 == 0:
            self._update_target_network()
    
    def _train_batch(self):
        """æ‰¹é‡è®­ç»ƒ"""
        batch = self.replay_buffer.sample(self.batch_size)
        
        states = np.array([e.state for e in batch])
        actions = np.array([e.action for e in batch])
        rewards = np.array([e.reward for e in batch])
        next_states = np.array([e.next_state for e in batch])
        dones = np.array([e.done for e in batch])
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        next_q_values = self.target_network.forward(next_states)
        max_next_q = np.max(next_q_values, axis=1)
        targets = rewards + self.gamma * max_next_q * (1 - dones)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q = self.q_network.forward(states)
        
        # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
        action_indices = np.array([self._discretize_action(a) for a in actions])
        loss_gradient = np.zeros_like(current_q)
        for i, idx in enumerate(action_indices):
            loss_gradient[i, idx] = current_q[i, idx] - targets[i]
        
        # åå‘ä¼ æ’­
        self.q_network.backward(loss_gradient / self.batch_size, self.learning_rate)
        
        # è®°å½•æŸå¤±
        loss = np.mean(loss_gradient ** 2)
        self.losses.append(loss)
    
    def get_policy_explanation(self, state: Dict) -> str:
        """
        è·å–ç­–ç•¥è§£é‡Š
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            ç­–ç•¥è§£é‡Šæ–‡æœ¬
        """
        action = self.select_action(
            np.array(list(state.values())[:self.state_dim]), 
            training=False
        )
        
        explanation = []
        explanation.append(f"å½“å‰ç­–ç•¥åˆ†æ:")
        
        battery_action = action['battery_action']
        if battery_action > 0.3:
            explanation.append(f"- ç”µæ± å……ç”µ (åŠŸç‡: {battery_action*100:.1f}%)")
            explanation.append("  åŸå› : å¯å†ç”Ÿèƒ½æºè¿‡å‰©æˆ–ç”µä»·è¾ƒä½")
        elif battery_action < -0.3:
            explanation.append(f"- ç”µæ± æ”¾ç”µ (åŠŸç‡: {abs(battery_action)*100:.1f}%)")
            explanation.append("  åŸå› : è´Ÿè·éœ€æ±‚é«˜æˆ–ç”µä»·è¾ƒé«˜")
        else:
            explanation.append("- ç”µæ± å¾…æœº")
            explanation.append("  åŸå› : ä¾›éœ€åŸºæœ¬å¹³è¡¡")
        
        if action['diesel_on']:
            explanation.append("- æŸ´æ²¹å‘ç”µæœº: å¼€å¯")
            explanation.append("  åŸå› : å¯å†ç”Ÿèƒ½æºå’Œå‚¨èƒ½ä¸è¶³ä»¥æ»¡è¶³è´Ÿè·")
        else:
            explanation.append("- æŸ´æ²¹å‘ç”µæœº: å…³é—­")
            explanation.append("  åŸå› : ä¼˜å…ˆä½¿ç”¨æ¸…æ´èƒ½æº")
        
        return "\n".join(explanation)
    
    def get_detailed_strategy_analysis(self, state_dict: Dict) -> Dict:
        """
        è·å–è¯¦ç»†çš„ç­–ç•¥åˆ†æ
        
        Args:
            state_dict: å®Œæ•´çš„ç³»ç»ŸçŠ¶æ€å­—å…¸
            
        Returns:
            è¯¦ç»†åˆ†æç»“æœå­—å…¸
        """
        # æå–å…³é”®çŠ¶æ€ä¿¡æ¯
        components = state_dict.get('components', {})
        weather = state_dict.get('weather', {})
        price = state_dict.get('price', {})
        statistics = state_dict.get('statistics', {})
        
        # è·å–å½“å‰å‚æ•°
        solar_power = components.get('solar', {}).get('current_power', 0)
        wind_power = components.get('wind', {}).get('current_power', 0)
        load_power = components.get('load', {}).get('current', 0)
        battery_soc = components.get('battery', {}).get('soc', 0.5)
        electricity_price = price.get('buy_price', 0.8)
        price_period = price.get('period', 'normal')
        
        renewable_power = solar_power + wind_power
        power_balance = renewable_power - load_power
        
        # æ„å»ºè§‚æµ‹å‘é‡è¿›è¡Œå†³ç­–
        obs = np.zeros(self.state_dim)
        obs[0] = 0.5  # å½’ä¸€åŒ–æ—¶é—´
        obs[1] = solar_power / 100 if solar_power > 0 else 0
        obs[2] = wind_power / 50 if wind_power > 0 else 0
        obs[3] = load_power / 150 if load_power > 0 else 0
        obs[4] = battery_soc
        obs[5] = electricity_price / 1.5
        
        # è·å–å†³ç­–
        action = self.select_action(obs, training=False)
        battery_action = action['battery_action']
        diesel_on = action['diesel_on']
        
        # è®¡ç®—å½±å“å› å­æƒé‡
        factors = self._analyze_decision_factors(
            solar_power, wind_power, load_power, battery_soc, 
            electricity_price, price_period, power_balance
        )
        
        # ç”Ÿæˆæ›¿ä»£æ–¹æ¡ˆåˆ†æ
        alternatives = self._generate_alternatives(
            battery_action, diesel_on, power_balance, battery_soc
        )
        
        # é¢„æœŸç»“æœè¯„ä¼°
        expected_outcomes = self._estimate_outcomes(
            battery_action, diesel_on, power_balance, 
            electricity_price, battery_soc
        )
        
        return {
            'current_conditions': {
                'solar_power': round(solar_power, 1),
                'wind_power': round(wind_power, 1),
                'renewable_total': round(renewable_power, 1),
                'load_power': round(load_power, 1),
                'power_balance': round(power_balance, 1),
                'battery_soc': round(battery_soc * 100, 1),
                'electricity_price': round(electricity_price, 2),
                'price_period': price_period
            },
            'decision': {
                'battery_action': round(battery_action, 2),
                'battery_action_type': self._get_battery_action_type(battery_action),
                'battery_power_percent': round(abs(battery_action) * 100, 1),
                'diesel_on': diesel_on
            },
            'factors': factors,
            'alternatives': alternatives,
            'expected_outcomes': expected_outcomes,
            'confidence': self._calculate_decision_confidence(factors)
        }
    
    def _analyze_decision_factors(self, solar: float, wind: float, load: float,
                                   soc: float, price: float, period: str,
                                   balance: float) -> Dict:
        """åˆ†æå½±å“å†³ç­–çš„å„ä¸ªå› ç´ """
        factors = {}
        
        # åŠŸç‡å¹³è¡¡å› ç´ 
        if balance > 20:
            factors['power_surplus'] = {
                'weight': min(1.0, balance / 50),
                'impact': 'positive',
                'description': f'å¯å†ç”Ÿèƒ½æºè¿‡å‰© {balance:.1f} kWï¼Œé€‚åˆå……ç”µæˆ–å”®ç”µ'
            }
        elif balance < -20:
            factors['power_deficit'] = {
                'weight': min(1.0, abs(balance) / 50),
                'impact': 'negative', 
                'description': f'åŠŸç‡ç¼ºå£ {abs(balance):.1f} kWï¼Œéœ€è¦æ”¾ç”µæˆ–è´­ç”µ'
            }
        else:
            factors['power_balanced'] = {
                'weight': 0.5,
                'impact': 'neutral',
                'description': 'ä¾›éœ€åŸºæœ¬å¹³è¡¡'
            }
        
        # ç”µæ± SOCå› ç´ 
        if soc < 0.2:
            factors['low_soc'] = {
                'weight': 0.9,
                'impact': 'critical',
                'description': f'ç”µæ± ç”µé‡è¿‡ä½ ({soc*100:.1f}%)ï¼Œéœ€è¦å……ç”µ'
            }
        elif soc > 0.8:
            factors['high_soc'] = {
                'weight': 0.8,
                'impact': 'limiting',
                'description': f'ç”µæ± æ¥è¿‘æ»¡å…… ({soc*100:.1f}%)ï¼Œé™åˆ¶å……ç”µ'
            }
        else:
            factors['normal_soc'] = {
                'weight': 0.3,
                'impact': 'neutral',
                'description': f'ç”µæ± ç”µé‡æ­£å¸¸ ({soc*100:.1f}%)'
            }
        
        # ç”µä»·å› ç´ 
        if period == 'valley':
            factors['valley_price'] = {
                'weight': 0.7,
                'impact': 'favorable',
                'description': f'ä½è°·ç”µä»· (Â¥{price:.2f}/kWh)ï¼Œé€‚åˆå……ç”µ'
            }
        elif period == 'peak':
            factors['peak_price'] = {
                'weight': 0.7,
                'impact': 'expensive',
                'description': f'é«˜å³°ç”µä»· (Â¥{price:.2f}/kWh)ï¼Œé€‚åˆæ”¾ç”µ'
            }
        else:
            factors['normal_price'] = {
                'weight': 0.4,
                'impact': 'neutral',
                'description': f'å¹³æ®µç”µä»· (Â¥{price:.2f}/kWh)'
            }
        
        # å¯å†ç”Ÿèƒ½æºå› ç´ 
        renewable_ratio = (solar + wind) / max(load, 1)
        if renewable_ratio > 0.8:
            factors['high_renewable'] = {
                'weight': 0.6,
                'impact': 'positive',
                'description': f'å¯å†ç”Ÿèƒ½æºå……è¶³ ({renewable_ratio*100:.0f}%è¦†ç›–)'
            }
        elif renewable_ratio < 0.3:
            factors['low_renewable'] = {
                'weight': 0.5,
                'impact': 'concerning',
                'description': f'å¯å†ç”Ÿèƒ½æºä¸è¶³ ({renewable_ratio*100:.0f}%è¦†ç›–)'
            }
        
        return factors
    
    def _get_battery_action_type(self, action: float) -> str:
        """è·å–ç”µæ± åŠ¨ä½œç±»å‹æè¿°"""
        if action > 0.5:
            return 'å¿«é€Ÿå……ç”µ'
        elif action > 0.2:
            return 'ä¸­é€Ÿå……ç”µ'
        elif action > 0:
            return 'æ…¢é€Ÿå……ç”µ'
        elif action > -0.2:
            return 'å¾…æœº/å¾®æ”¾ç”µ'
        elif action > -0.5:
            return 'ä¸­é€Ÿæ”¾ç”µ'
        else:
            return 'å¿«é€Ÿæ”¾ç”µ'
    
    def _generate_alternatives(self, current_action: float, diesel_on: bool,
                                balance: float, soc: float) -> List[Dict]:
        """ç”Ÿæˆæ›¿ä»£æ–¹æ¡ˆåˆ†æ"""
        alternatives = []
        
        # æ›´æ¿€è¿›çš„å……ç”µ
        if current_action < 0.8 and balance > 0 and soc < 0.8:
            alternatives.append({
                'action': 'å¢åŠ å……ç”µåŠŸç‡',
                'pros': ['æ›´å¥½åœ°åˆ©ç”¨è¿‡å‰©å¯å†ç”Ÿèƒ½æº', 'å‚¨å¤‡æ›´å¤šèƒ½é‡'],
                'cons': ['ç”µæ± ç£¨æŸåŠ é€Ÿ', 'å¯èƒ½å¯¼è‡´è¿‡å……'],
                'recommendation': 'moderate'
            })
        
        # æ›´æ¿€è¿›çš„æ”¾ç”µ
        if current_action > -0.8 and balance < 0 and soc > 0.3:
            alternatives.append({
                'action': 'å¢åŠ æ”¾ç”µåŠŸç‡',
                'pros': ['å‡å°‘è´­ç”µæˆæœ¬', 'æé«˜è‡ªç»™ç‡'],
                'cons': ['ç”µæ± ç”µé‡ä¸‹é™å¿«', 'å¯èƒ½å½±å“åç»­ä¾›ç”µ'],
                'recommendation': 'moderate'
            })
        
        # å¯åŠ¨æŸ´æ²¹æœº
        if not diesel_on and balance < -30 and soc < 0.3:
            alternatives.append({
                'action': 'å¯åŠ¨æŸ´æ²¹å‘ç”µæœº',
                'pros': ['ä¿è¯ä¾›ç”µå¯é æ€§', 'ç¼“è§£åŠŸç‡ç¼ºå£'],
                'cons': ['å¢åŠ ç¢³æ’æ”¾', 'è¿è¡Œæˆæœ¬é«˜'],
                'recommendation': 'emergency_only'
            })
        
        # ç»´æŒç°çŠ¶
        alternatives.append({
            'action': 'ç»´æŒå½“å‰ç­–ç•¥',
            'pros': ['ç¨³å®šè¿è¡Œ', 'é£é™©å¯æ§'],
            'cons': ['å¯èƒ½éæœ€ä¼˜'],
            'recommendation': 'default'
        })
        
        return alternatives
    
    def _estimate_outcomes(self, battery_action: float, diesel_on: bool,
                           balance: float, price: float, soc: float) -> Dict:
        """ä¼°è®¡é¢„æœŸç»“æœ"""
        # ä¼°è®¡æ¯å°æ—¶æˆæœ¬
        if balance > 0:
            # æœ‰ç›ˆä½™,å¯èƒ½å”®ç”µ
            export_power = min(balance, 50)  # å‡è®¾æœ€å¤§å”®ç”µ50kW
            hourly_revenue = export_power * price * 0.7
            hourly_cost = 0
        else:
            # æœ‰ç¼ºå£,éœ€è¦è´­ç”µæˆ–æ”¾ç”µ
            deficit = abs(balance)
            battery_contribution = min(deficit, abs(battery_action) * 50) if battery_action < 0 else 0
            grid_import = deficit - battery_contribution
            hourly_cost = grid_import * price
            hourly_revenue = 0
        
        # ä¼°è®¡SOCå˜åŒ–
        if battery_action > 0:
            soc_change = battery_action * 0.25  # å‡è®¾æ»¡åŠŸç‡å……ç”µæ¯å°æ—¶25%
        else:
            soc_change = battery_action * 0.25
        
        new_soc = max(0.1, min(0.9, soc + soc_change))
        
        return {
            'estimated_hourly_cost': round(hourly_cost, 2),
            'estimated_hourly_revenue': round(hourly_revenue, 2),
            'net_cost': round(hourly_cost - hourly_revenue, 2),
            'soc_after_1h': round(new_soc * 100, 1),
            'grid_dependency': 'ä½' if abs(balance) < 20 else ('ä¸­' if abs(balance) < 50 else 'é«˜'),
            'renewable_utilization': 'é«˜' if balance >= 0 else 'ä¸­ç­‰'
        }
    
    def _calculate_decision_confidence(self, factors: Dict) -> Dict:
        """è®¡ç®—å†³ç­–ç½®ä¿¡åº¦"""
        total_weight = sum(f['weight'] for f in factors.values())
        positive_weight = sum(f['weight'] for f in factors.values() 
                             if f['impact'] in ['positive', 'favorable'])
        negative_weight = sum(f['weight'] for f in factors.values() 
                             if f['impact'] in ['negative', 'critical', 'expensive'])
        
        # åŸºäºå› ç´ ä¸€è‡´æ€§è®¡ç®—ç½®ä¿¡åº¦
        if total_weight > 0:
            consistency = 1 - abs(positive_weight - negative_weight) / total_weight
            base_confidence = 0.5 + 0.3 * (1 - consistency)
        else:
            base_confidence = 0.5
        
        # è€ƒè™‘æ¢ç´¢ç‡
        exploration_factor = 1 - self.epsilon
        final_confidence = base_confidence * exploration_factor
        
        return {
            'level': round(final_confidence * 100, 1),
            'description': self._get_confidence_description(final_confidence),
            'exploration_mode': self.epsilon > 0.1
        }
    
    def _get_confidence_description(self, confidence: float) -> str:
        """è·å–ç½®ä¿¡åº¦æè¿°"""
        if confidence > 0.8:
            return 'é«˜ç½®ä¿¡åº¦ - å†³ç­–å› ç´ æ˜ç¡®ä¸€è‡´'
        elif confidence > 0.6:
            return 'ä¸­ç­‰ç½®ä¿¡åº¦ - å­˜åœ¨ä¸€å®šæƒè¡¡'
        elif confidence > 0.4:
            return 'è¾ƒä½ç½®ä¿¡åº¦ - å¤šå› ç´ ç›¸äº’åˆ¶çº¦'
        else:
            return 'ä½ç½®ä¿¡åº¦ - å¤„äºæ¢ç´¢å­¦ä¹ é˜¶æ®µ'
    
    def format_strategy_display(self, state_dict: Dict) -> str:
        """
        æ ¼å¼åŒ–ç­–ç•¥æ˜¾ç¤ºï¼ˆç”¨äºç•Œé¢å±•ç¤ºï¼‰
        
        Args:
            state_dict: å®Œæ•´çš„ç³»ç»ŸçŠ¶æ€å­—å…¸
            
        Returns:
            æ ¼å¼åŒ–çš„ç­–ç•¥åˆ†ææ–‡æœ¬
        """
        analysis = self.get_detailed_strategy_analysis(state_dict)
        
        lines = []
        lines.append("=" * 55)
        lines.append("        ğŸ§  æ™ºèƒ½èƒ½é‡ç®¡ç†ç­–ç•¥è¯¦ç»†åˆ†æ")
        lines.append("=" * 55)
        lines.append("")
        
        # å½“å‰çŠ¶å†µ
        cond = analysis['current_conditions']
        lines.append("ğŸ“Š ã€å½“å‰ç³»ç»ŸçŠ¶å†µã€‘")
        lines.append(f"   â˜€ï¸ å…‰ä¼å‘ç”µ: {cond['solar_power']:.1f} kW")
        lines.append(f"   ğŸ’¨ é£åŠ›å‘ç”µ: {cond['wind_power']:.1f} kW")
        lines.append(f"   ğŸŒ¿ å¯å†ç”Ÿæ€»è®¡: {cond['renewable_total']:.1f} kW")
        lines.append(f"   ğŸ“ˆ è´Ÿè·éœ€æ±‚: {cond['load_power']:.1f} kW")
        lines.append(f"   âš–ï¸ åŠŸç‡å¹³è¡¡: {cond['power_balance']:+.1f} kW")
        lines.append(f"   ğŸ”‹ ç”µæ± SOC: {cond['battery_soc']:.1f}%")
        lines.append(f"   ğŸ’° å½“å‰ç”µä»·: Â¥{cond['electricity_price']:.2f}/kWh ({cond['price_period']})")
        lines.append("")
        
        # å†³ç­–ç»“æœ
        dec = analysis['decision']
        lines.append("ğŸ¯ ã€ç­–ç•¥å†³ç­–ã€‘")
        lines.append(f"   ç”µæ± æ“ä½œ: {dec['battery_action_type']}")
        lines.append(f"   æ§åˆ¶åŠŸç‡: {dec['battery_power_percent']:.0f}% é¢å®šåŠŸç‡")
        lines.append(f"   æŸ´æ²¹å‘ç”µ: {'å¯åŠ¨' if dec['diesel_on'] else 'å…³é—­'}")
        lines.append("")
        
        # å†³ç­–å› ç´ 
        lines.append("ğŸ” ã€å†³ç­–å› ç´ åˆ†æã€‘")
        for name, factor in analysis['factors'].items():
            impact_icon = {
                'positive': 'âœ…', 'favorable': 'âœ…',
                'negative': 'âš ï¸', 'critical': 'ğŸ”´', 'expensive': 'ğŸ’¸',
                'neutral': 'â–', 'limiting': 'â›”', 'concerning': 'âš¡'
            }.get(factor['impact'], 'â€¢')
            lines.append(f"   {impact_icon} {factor['description']}")
            lines.append(f"      æƒé‡: {'â–ˆ' * int(factor['weight'] * 10)}{'â–‘' * (10 - int(factor['weight'] * 10))} ({factor['weight']:.1f})")
        lines.append("")
        
        # é¢„æœŸç»“æœ
        out = analysis['expected_outcomes']
        lines.append("ğŸ“ˆ ã€é¢„æœŸæ•ˆæœï¼ˆ1å°æ—¶ï¼‰ã€‘")
        lines.append(f"   é¢„è®¡æˆæœ¬: Â¥{out['net_cost']:.2f}")
        lines.append(f"   ç”µæ± SOC: â†’ {out['soc_after_1h']:.1f}%")
        lines.append(f"   ç”µç½‘ä¾èµ–: {out['grid_dependency']}")
        lines.append(f"   æ¸…æ´èƒ½æºåˆ©ç”¨: {out['renewable_utilization']}")
        lines.append("")
        
        # ç½®ä¿¡åº¦
        conf = analysis['confidence']
        lines.append("ğŸ² ã€å†³ç­–ç½®ä¿¡åº¦ã€‘")
        conf_bar = 'â–ˆ' * int(conf['level'] / 10) + 'â–‘' * (10 - int(conf['level'] / 10))
        lines.append(f"   ç½®ä¿¡åº¦: {conf_bar} {conf['level']:.0f}%")
        lines.append(f"   {conf['description']}")
        if conf['exploration_mode']:
            lines.append(f"   âš¡ å½“å‰å¤„äºæ¢ç´¢å­¦ä¹ æ¨¡å¼")
        lines.append("")
        
        # æ›¿ä»£æ–¹æ¡ˆ
        lines.append("ğŸ’¡ ã€æ›¿ä»£æ–¹æ¡ˆå‚è€ƒã€‘")
        for alt in analysis['alternatives'][:3]:
            rec_icon = {'moderate': 'ğŸ”¸', 'emergency_only': 'ğŸ”º', 'default': 'ğŸ”¹'}.get(
                alt['recommendation'], 'â€¢')
            lines.append(f"   {rec_icon} {alt['action']}")
            lines.append(f"      ä¼˜ç‚¹: {', '.join(alt['pros'][:2])}")
            lines.append(f"      ç¼ºç‚¹: {', '.join(alt['cons'][:2])}")
        
        lines.append("")
        lines.append("=" * 55)
        
        return "\n".join(lines)
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        data = {
            'q_network': self.q_network.get_params(),
            'epsilon': self.epsilon,
            'training_steps': self.training_steps,
            'episode_rewards': self.episode_rewards,
            'config': {
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'gamma': self.gamma,
                'learning_rate': self.learning_rate
            }
        }
        with open(path, 'w') as f:
            json.dump(data, f)
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        with open(path, 'r') as f:
            data = json.load(f)
        self.q_network.set_params(data['q_network'])
        self.epsilon = data.get('epsilon', self.epsilon_min)
        self.training_steps = data.get('training_steps', 0)
        self._update_target_network()


class RuleBasedAgent:
    """åŸºäºè§„åˆ™çš„èƒ½é‡ç®¡ç†æ™ºèƒ½ä½“ï¼ˆä½œä¸ºåŸºå‡†ï¼‰"""
    
    def __init__(self, config: Optional[Dict] = None):
        config = config or {}
        self.soc_high_threshold = config.get('soc_high', 0.8)
        self.soc_low_threshold = config.get('soc_low', 0.3)
        self.price_high_threshold = config.get('price_high', 1.0)
        self.price_low_threshold = config.get('price_low', 0.5)
        
    def select_action(self, state: Dict) -> Dict:
        """åŸºäºè§„åˆ™é€‰æ‹©åŠ¨ä½œ"""
        soc = state.get('battery_soc', 0.5)
        price = state.get('electricity_price', 0.8)
        solar = state.get('solar_power', 0)
        wind = state.get('wind_power', 0)
        load = state.get('load_power', 100)
        
        renewable = solar + wind
        net_power = renewable - load
        
        battery_action = 0.0
        diesel_on = False
        
        # è§„åˆ™1: å¯å†ç”Ÿèƒ½æºè¿‡å‰©æ—¶å……ç”µ
        if net_power > 20 and soc < self.soc_high_threshold:
            battery_action = min(1.0, net_power / 50)
        
        # è§„åˆ™2: ä½ç”µä»·æ—¶å……ç”µ
        elif price < self.price_low_threshold and soc < 0.7:
            battery_action = 0.5
        
        # è§„åˆ™3: é«˜ç”µä»·æ—¶æ”¾ç”µ
        elif price > self.price_high_threshold and soc > self.soc_low_threshold:
            battery_action = -0.5
        
        # è§„åˆ™4: è´Ÿè·ä¸è¶³æ—¶æ”¾ç”µ
        elif net_power < -30 and soc > self.soc_low_threshold:
            battery_action = max(-1.0, net_power / 50)
        
        # è§„åˆ™5: ç´§æ€¥æƒ…å†µå¯åŠ¨æŸ´æ²¹æœº
        if net_power < -50 and soc < 0.2:
            diesel_on = True
        
        return {
            'battery_action': battery_action,
            'diesel_on': diesel_on
        }


class AdaptiveEnergyManager:
    """è‡ªé€‚åº”èƒ½é‡ç®¡ç†ç³»ç»Ÿ"""
    
    def __init__(self):
        self.rl_agent = EnergyManagementAgent()
        self.rule_agent = RuleBasedAgent()
        
        self.mode = 'hybrid'  # 'rl', 'rule', 'hybrid'
        self.rl_confidence = 0.5
        self.performance_history = deque(maxlen=100)
        
    def select_action(self, state: np.ndarray, state_dict: Dict,
                      training: bool = True) -> Dict:
        """é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ"""
        if self.mode == 'rule':
            return self.rule_agent.select_action(state_dict)
        elif self.mode == 'rl':
            return self.rl_agent.select_action(state, training)
        else:  # hybrid
            rl_action = self.rl_agent.select_action(state, training)
            rule_action = self.rule_agent.select_action(state_dict)
            
            # æ··åˆåŠ¨ä½œ
            battery_action = (
                self.rl_confidence * rl_action['battery_action'] +
                (1 - self.rl_confidence) * rule_action['battery_action']
            )
            diesel_on = rl_action['diesel_on'] or rule_action['diesel_on']
            
            return {
                'battery_action': battery_action,
                'diesel_on': diesel_on
            }
    
    def update_confidence(self, reward: float):
        """æ ¹æ®è¡¨ç°æ›´æ–°RLç½®ä¿¡åº¦"""
        self.performance_history.append(reward)
        
        if len(self.performance_history) >= 50:
            recent_performance = np.mean(list(self.performance_history)[-50:])
            if recent_performance > 0:
                self.rl_confidence = min(0.95, self.rl_confidence + 0.01)
            else:
                self.rl_confidence = max(0.3, self.rl_confidence - 0.01)
    
    def train(self, state: np.ndarray, action: Dict, 
              reward: float, next_state: np.ndarray, done: bool):
        """è®­ç»ƒRLæ™ºèƒ½ä½“"""
        self.rl_agent.train_step(state, action, reward, next_state, done)
        self.update_confidence(reward)
    
    def get_status(self) -> Dict:
        """è·å–ç®¡ç†å™¨çŠ¶æ€"""
        return {
            'mode': self.mode,
            'rl_confidence': self.rl_confidence,
            'epsilon': self.rl_agent.epsilon,
            'training_steps': self.rl_agent.training_steps,
            'buffer_size': len(self.rl_agent.replay_buffer),
            'recent_performance': np.mean(list(self.performance_history)) if self.performance_history else 0
        }
